{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda = False\n",
    "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imsize = 512 if use_cuda else 128\n",
    "loader = transforms.Compose([transforms.Resize(imsize), transforms.ToTensor()])\n",
    "\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = Variable(loader(image))\n",
    "    # fake batch demension to feed into the network\n",
    "\n",
    "    image = image.unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_filename = \"./images/picasso.jpg\"\n",
    "content_filename = \"./images/dancing.jpg\"\n",
    "\n",
    "import os.path\n",
    "assert os.path.isfile(style_filename) and os.path.isfile(content_filename), \"Content or style image do not exist.\"\n",
    "\n",
    "\n",
    "style_img = image_loader(style_filename).type(dtype)\n",
    "content_img = image_loader(content_filename).type(dtype)\n",
    "\n",
    "assert style_img.size() == content_img.size(), \\\n",
    "    \"content and style image need to have same size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unloader = transforms.ToPILImage()\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.clone().cpu()\n",
    "    image = image.view(3, imsize, imsize) # remove fake batch dimension\n",
    "    image = unloader(image)\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001) # pause a bit to update the plots\n",
    "    \n",
    "plt.figure()\n",
    "imshow(style_img.data, title=\"Style Image\")\n",
    "\n",
    "plt.figure()\n",
    "imshow(content_img.data, title=\"Content Image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target, weight):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target.detach() * weight\n",
    "        self.weight = weight\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.loss = self.criterion(input * self.weight, self.target)\n",
    "        self.output = input\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, retain_graph=True):\n",
    "        self.loss.backward(retain_graph=retain_graph)\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammMatrix(nn.Module):\n",
    "    \n",
    "    def forward(self, input):\n",
    "        a, b, c, d = input.size() # a=batch_size, b=feature_maps, c&d=dimensions\n",
    "        features = input.view(a*b, c*d)\n",
    "        G = torch.mm(features, features.t())\n",
    "        \n",
    "        #normalize\n",
    "        return G.div(a*b*c*d)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, target, weight):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = target.detach() * weight\n",
    "        self.weight = weight\n",
    "        self.gramm = GrammMatrix()\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.output = input.clone()\n",
    "        self.G = self.gramm(input)\n",
    "        self.G.mul_(self.weight)\n",
    "        self.loss = self.criterion(self.G, self.target)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, retain_graph=True):\n",
    "        self.loss.backward(retain_graph=retain_graph)\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = models.vgg19(pretrained=True).features\n",
    "\n",
    "if use_cuda:\n",
    "    cnn = cnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers_default = ['conv_4']\n",
    "style_layers_default = ['conv_1','conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "def get_style_model_and_losses(cnn, style_img, content_img, \n",
    "                               style_weight=1000, content_weight=1, \n",
    "                               content_layers=content_layers_default,\n",
    "                              style_layers=style_layers_default):\n",
    "    \n",
    "    cnn = copy.deepcopy(cnn)\n",
    "    \n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "    \n",
    "    model = nn.Sequential()\n",
    "    gram = GrammMatrix()\n",
    "    \n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        gram = gram.cuda()\n",
    "        \n",
    "    i = 1\n",
    "    for layer in list(cnn):\n",
    "        \n",
    "        is_conv_layer = isinstance(layer, nn.Conv2d)\n",
    "        is_relu_layer = isinstance(layer, nn.ReLU)\n",
    "        is_maxpool_layer = isinstance(layer, nn.MaxPool2d)\n",
    "        \n",
    "        if is_conv_layer:\n",
    "            name_prefix = \"conv_\"\n",
    "        elif is_relu_layer:\n",
    "            name_prefix = \"relu_\"\n",
    "            i += 1\n",
    "        elif is_maxpool_layer:\n",
    "            name_prefix = \"pool_\"\n",
    "        \n",
    "        \n",
    "        if is_conv_layer or is_relu_layer:\n",
    "            name = name_prefix + str(i)\n",
    "            model.add_module(name, layer)\n",
    "            \n",
    "            if name in content_layers:\n",
    "                target = model(content_img).clone()\n",
    "                content_loss = ContentLoss(target, content_weight)\n",
    "                model.add_module(\"content_loss_\"+str(i), content_loss)\n",
    "                content_losses.append(content_loss)\n",
    "                \n",
    "            if name in style_layers:\n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gramm = gram(target_feature)\n",
    "                style_loss = StyleLoss(target_feature_gramm, style_weight)\n",
    "                model.add_module(\"style_loss_\"+str(i), style_loss)\n",
    "                style_losses.append(style_loss)\n",
    "                \n",
    "    return model, style_losses, content_losses\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = content_img.clone()\n",
    "# input_img = Variable(torch.randn(content_img.data.size())).type(dtype)\n",
    "plt.figure()\n",
    "imshow(input_img.data, title=\"Input Image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_param_optimizer(input_img):\n",
    "    input_param = nn.Parameter(input_img.data)\n",
    "    optimizer = optim.LBFGS([input_param])\n",
    "    return input_param, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_style_transfer(cnn, content_img, style_img, input_img, num_steps=300,\n",
    "                      style_weight=1000, content_weight=1):\n",
    "    \"\"\"Run the style transfer\"\"\"\n",
    "    print(\"Building the style transfer model..\")\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(cnn, style_img, content_img, style_weight, content_weight)\n",
    "    input_param, optimizer = input_param_optimizer(input_img)\n",
    "    \n",
    "    print(\"Optimizing..\")\n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "        \n",
    "        def closure():\n",
    "            input_param.data.clamp_(0,1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            model(input_param)\n",
    "            style_score = 0\n",
    "            content_score = 0\n",
    "            for sl in style_losses:\n",
    "                style_score += sl.backward()\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.backward()\n",
    "            \n",
    "            run[0] += 1\n",
    "            if run[0] % 1 == 0:\n",
    "                print(\"run {}:\".format(run))\n",
    "                print(\"Style Loss: {:4f} Content Loss: {:4f}\".format(style_score.data[0], content_score.data[0]))\n",
    "                \n",
    "            return style_score + content_score\n",
    "        \n",
    "        optimizer.step(closure)\n",
    "        \n",
    "    input_param.data.clamp_(0,1)\n",
    "    \n",
    "    return input_param.data\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = run_style_transfer(cnn, content_img, style_img, input_img)\n",
    "\n",
    "plt.figure()\n",
    "imshow(output, title=\"Output Image\")\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
